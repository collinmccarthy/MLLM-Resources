# MLLM-Resources

I will try to organize this more in the future, for now it's just a bunch of links as follows.

## Attention

- [Jay Alammar: The Illustrated Transformer (Blog)](https://jalammar.github.io/illustrated-transformer/)
- [Lilian Weng: Attention? Attention! (Blog)](https://lilianweng.github.io/posts/2018-06-24-attention/)
- [Jay Mody: An Intuition for Attention (Blog)](https://jaykmody.com/blog/attention-intuition)
- [Jay Alammar: Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) (Blog)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

## Decoder-Only LLMs (Generative)

- [Jay Alammar: The Illustrated GPT-2 (Blog)](https://jalammar.github.io/illustrated-gpt2/)
- [Jay Mody: GPT in 60 Lines of Numpy (Blog)](https://jaykmody.com/blog/gpt-from-scratch/)
- [Umar Jamil: LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU (Video)](https://youtu.be/Mn_9W1nCFLo?si=ENL35JQa0KDwsTzt)

## Encoder-Only LLMs

- [Jay Alammar: The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) (Blog)](https://jalammar.github.io/illustrated-bert/)

## Word Embeddings

- [Jay Alammar: The Illustrated Word2vec (Blog)](https://jalammar.github.io/illustrated-word2vec/)

## Performance

- [Dipkumar Patel: Speeding up the GPT - KV cache (Blog)](https://dipkumar.dev/becoming-the-unbeatable/posts/gpt-kvcache/)
- [Lilian Weng: Large Transformer Model Inference Optimization (Blog)](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
- [Carol Chen: Transformer Inference Arithmetic (Blog)](https://kipp.ly/transformer-inference-arithmetic/)

## DeepSpeed

- [Large Model Training and Inference with DeepSpeed (Video)](https://www.youtube.com/watch?v=cntxC3g22oU&t=1754s&pp=ygUJZGVlcHNwZWVk)
- [Zero Redundancy Optimizer (Docs)](https://www.deepspeed.ai/tutorials/zero/)
- [ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters (Blog, Feb 2020)](https://www.microsoft.com/en-us/research/blog/ZeRO-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)
- [ZeRO-2 & DeepSpeed: Shattering barriers of deep learning speed & scale (Blog, May 2020)](https://www.microsoft.com/en-us/research/blog/ZeRO-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/)
- [DeepSpeed ZeRO-3 Offload (Docs, Mar 2021)](https://www.deepspeed.ai/2021/03/07/zero3-offload.html)
- [DeepSpeed ZeRO++: A leap in speed for LLM and chat model training with 4X less communication (Blog, June 2023)](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/)

## HuggingFace

- [Trainer (Docs)](https://huggingface.co/docs/transformers/en/trainer)
