# MLLM-Resources

I will try to organize this more in the future, for now it's just a bunch of links as follows.

## Attention

- [Jay Alammar: The Illustrated Transformer (Blog)](https://jalammar.github.io/illustrated-transformer/)
- [Lilian Weng: Attention? Attention! (Blog)](https://lilianweng.github.io/posts/2018-06-24-attention/)
- [Jay Mody: An Intuition for Attention (Blog)](https://jaykmody.com/blog/attention-intuition)
- [Jay Alammar: Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) (Blog)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

## Decoder-Only LLMs (Generative)

- [Jay Alammar: The Illustrated GPT-2 (Blog)](https://jalammar.github.io/illustrated-gpt2/)
- [Jay Mody: GPT in 60 Lines of Numpy (Blog)](https://jaykmody.com/blog/gpt-from-scratch/)
- [Umar Jamil: LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU (Video)](https://youtu.be/Mn_9W1nCFLo?si=ENL35JQa0KDwsTzt)

## Encoder-Only LLMs

- [Jay Alammar: The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) (Blog)](https://jalammar.github.io/illustrated-bert/)

## Word Embeddings

- [Jay Alammar: The Illustrated Word2vec (Blog)](https://jalammar.github.io/illustrated-word2vec/)

## Performance

- [Dipkumar Patel: Speeding up the GPT - KV cache (Blog)](https://dipkumar.dev/becoming-the-unbeatable/posts/gpt-kvcache/)
- [Lilian Weng: Large Transformer Model Inference Optimization (Blog)](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
- [Carol Chen: Transformer Inference Arithmetic (Blog)](https://kipp.ly/transformer-inference-arithmetic/)
