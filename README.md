# MLLM-Resources

I will try to organize this more in the future, for now it's just a bunch of links as follows.

## Attention

- [Jay Alammar: The Illustrated Transformer (Blog)](https://jalammar.github.io/illustrated-transformer/)
- [Lilian Weng: Attention? Attention! (Blog)](https://lilianweng.github.io/posts/2018-06-24-attention/)
- [Jay Mody: An Intuition for Attention (Blog)](https://jaykmody.com/blog/attention-intuition)

## Generative LLMs

- [Jay Alammar: The Illustrated GPT-2 (Blog)](https://jalammar.github.io/illustrated-gpt2/)
- [Jay Mody: GPT in 60 Lines of Numpy (Blog)](https://jaykmody.com/blog/gpt-from-scratch/)
- [Umar Jamil: LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU (Video)](https://youtu.be/Mn_9W1nCFLo?si=ENL35JQa0KDwsTzt)

## Performance

- [Dipkumar Patel: Speeding up the GPT - KV cache (Blog)](https://dipkumar.dev/becoming-the-unbeatable/posts/gpt-kvcache/)
- [Lilian Weng: Large Transformer Model Inference Optimization (Blog)](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
- [Carol Chen: Transformer Inference Arithmetic (Blog)](https://kipp.ly/transformer-inference-arithmetic/)
